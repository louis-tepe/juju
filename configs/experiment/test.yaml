# @package _global_
# Mode Test : Optimized for MacBook Pro M2 (16GB RAM)
# Two-phase training: Head warmup â†’ Full fine-tuning

defaults:
  - override /model: efficientnet

# Data settings (M2 optimized)
data:
  image_size: 224
  batch_size: 64
  use_ben_graham: false
  use_native_augment: true
  use_cache: true

# Model
model:
  backbone: "efficientnet_b0"
  dropout: 0.3

# Two-phase training
train:
  epochs: 10
  warmup_epochs: 3 # Phase 1: head only (frozen backbone)
  lr: 1e-3 # Phase 1 LR
  finetune_lr: 1e-4 # Phase 2 LR (10x lower)
  patience: 5 # More patience for fine-tuning
  fold: 0
  use_mixed_precision: false
  use_xla: false
  min_lr: 1e-6
